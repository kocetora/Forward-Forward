{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "import math\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "XhtHj_mVLFPe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "train_images = train_images.reshape((train_images.shape[0], 784))\n",
        "test_images = test_images.reshape((test_images.shape[0], 784))\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    train_images, train_labels, test_size=0.99, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "itplZCtCLPJQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ReLU, SSE and Logistic functions\n",
        "def logistic(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sum_of_squared_relu(x):\n",
        "    return np.sum(relu(x)**2)\n",
        "\n",
        "def derivative_sum_of_squared_relu(x):\n",
        "    return 2 * relu(x) * (x > 0)"
      ],
      "metadata": {
        "id": "f6PSfjMiLQte"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_lables(input_data, class_index, num_classes=10):\n",
        "    modified_input = np.copy(input_data)\n",
        "    class_vector = np.zeros(num_classes)\n",
        "    class_vector[class_index] = 1\n",
        "    modified_input[:10] = class_vector  # Replace first 10 pixels\n",
        "    return modified_input"
      ],
      "metadata": {
        "id": "1b25LQwELTKV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize network\n",
        "def initialize_network(layer_sizes):\n",
        "    return [\n",
        "        {\n",
        "            'weights': np.random.rand(layer_sizes[i], layer_sizes[i+1]),\n",
        "            'biases': np.random.rand(layer_sizes[i+1])\n",
        "        }\n",
        "        for i in range(len(layer_sizes) - 1)\n",
        "    ]\n",
        "\n",
        "initialize_network([2,1,2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I5TJ_4eLVn5",
        "outputId": "b3ad33be-0f50-4520-ab4d-60fae8cef3e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'weights': array([[0.37454012],\n",
              "         [0.95071431]]),\n",
              "  'biases': array([0.73199394])},\n",
              " {'weights': array([[0.59865848, 0.15601864]]),\n",
              "  'biases': array([0.15599452, 0.05808361])}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_image(image_vector):\n",
        "    image_matrix = image_vector.reshape(28, 28)  # Reshape from 784 to 28x28\n",
        "    plt.imshow(image_matrix, cmap='gray')\n",
        "    plt.colorbar()\n",
        "    plt.show()\n",
        "\n",
        "# display_image(encode_lables(test_images[0], np.argmax(train_labels[0])))"
      ],
      "metadata": {
        "id": "TBoLJzIv4SxU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def l2norm(vector):\n",
        "    return vector / np.linalg.norm(vector)"
      ],
      "metadata": {
        "id": "AL7u0OhsKOaY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(network, val_image, num_classes = 10):\n",
        "    predictions = []\n",
        "    # display_image(val_image)\n",
        "    for image_class in range(num_classes):\n",
        "        # print(image_class)\n",
        "        prediction = []\n",
        "        inputs = encode_lables(val_image, image_class)\n",
        "        for layer in network:\n",
        "            inputs = np.dot(inputs, layer['weights']) + layer['biases']\n",
        "            prediction.append(logistic(sum_of_squared_relu(inputs)))\n",
        "        # print(image_class, ': ', np.mean(prediction), ': ', prediction)\n",
        "        predictions.append(np.mean(prediction))\n",
        "    # print('Predicted class: ', np.argmax(predictions))\n",
        "    return np.argmax(predictions)\n",
        "\n",
        "def accuracy(network, images, labels):\n",
        "    predictions = []\n",
        "    for i in range(len(images)):\n",
        "        predicted_class = predict(network, images[i:i+1][0])\n",
        "        real_class = np.argmax(labels[i:i+1], axis=1)[0]\n",
        "        predictions.append(predicted_class == real_class)\n",
        "        # print(predicted_class)\n",
        "        # print(real_class)\n",
        "        # print(predictions)\n",
        "    return np.mean(predictions)"
      ],
      "metadata": {
        "id": "MLYtWEStKAck"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagate with detailed logging\n",
        "def train_network(network, train_images, train_labels, val_images, val_labels, epochs, learning_rate):\n",
        "    for epoch in range(epochs):\n",
        "        # Training loop\n",
        "        for i in range(len(train_images)):\n",
        "            for l, layer in enumerate(network):\n",
        "                # positive data\n",
        "                if l == 0:\n",
        "                    image_class = np.argmax(train_labels[i:i+1], axis=1)[0]\n",
        "                    pos_inputs = encode_lables(train_images[i:i+1][0], image_class)\n",
        "                goodness = sum_of_squared_relu(np.dot(pos_inputs, layer['weights']) + layer['biases'])\n",
        "                grad = derivative_sum_of_squared_relu(np.dot(pos_inputs, layer['weights']) + layer['biases'])\n",
        "                layer['weights'] += learning_rate * grad\n",
        "                layer['biases'] += learning_rate * grad\n",
        "                # negative data\n",
        "                if l == 0:\n",
        "                    not_image_class = random.choice([i for i in range(10) if i != image_class])\n",
        "                    neg_inputs = encode_lables(train_images[i:i+1][0], not_image_class)\n",
        "                goodness = sum_of_squared_relu(np.dot(neg_inputs, layer['weights']) + layer['biases'])\n",
        "                grad = derivative_sum_of_squared_relu(np.dot(neg_inputs, layer['weights']) + layer['biases'])\n",
        "                layer['weights'] -= learning_rate * grad\n",
        "                layer['biases'] -= learning_rate * grad\n",
        "\n",
        "                pos_inputs = l2norm(np.dot(pos_inputs, layer['weights']) + layer['biases'])\n",
        "                neg_inputs = l2norm(np.dot(neg_inputs, layer['weights']) + layer['biases'])\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        train_accuracy = accuracy(network, train_images, train_labels)\n",
        "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "        val_accuracy = accuracy(network, val_images[:60], val_labels[:60])\n",
        "        print(f\"Test Accuracy: {val_accuracy:.4f}\")\n",
        "    # predict(network, val_images[0])\n",
        "\n",
        "learning_rate = 0.002\n",
        "epochs = 20\n",
        "network = initialize_network([784, 128, 64, 10])\n",
        "train_network(network, train_images, train_labels, val_images, val_labels, epochs, learning_rate)\n",
        "final_accuracy = accuracy(network, test_images[:60], test_labels[:60])\n",
        "print(\"Final Accuracy:\", final_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcfPG7m5J4HU",
        "outputId": "5ddb4bba-c09d-4c41-dda9-505ca34a02bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Training Accuracy: 0.1617\n",
            "Test Accuracy: 0.1667\n",
            "Epoch 2/20\n",
            "Training Accuracy: 0.1733\n",
            "Test Accuracy: 0.1667\n",
            "Epoch 3/20\n",
            "Training Accuracy: 0.1900\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 4/20\n",
            "Training Accuracy: 0.1850\n",
            "Test Accuracy: 0.2000\n",
            "Epoch 5/20\n",
            "Training Accuracy: 0.1817\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 6/20\n",
            "Training Accuracy: 0.1817\n",
            "Test Accuracy: 0.1667\n",
            "Epoch 7/20\n",
            "Training Accuracy: 0.1950\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 8/20\n",
            "Training Accuracy: 0.1950\n",
            "Test Accuracy: 0.2000\n",
            "Epoch 9/20\n",
            "Training Accuracy: 0.2100\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 10/20\n",
            "Training Accuracy: 0.1933\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 11/20\n",
            "Training Accuracy: 0.2067\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 12/20\n",
            "Training Accuracy: 0.1983\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 13/20\n",
            "Training Accuracy: 0.2100\n",
            "Test Accuracy: 0.2000\n",
            "Epoch 14/20\n",
            "Training Accuracy: 0.2000\n",
            "Test Accuracy: 0.2000\n",
            "Epoch 15/20\n",
            "Training Accuracy: 0.1900\n",
            "Test Accuracy: 0.2000\n",
            "Epoch 16/20\n",
            "Training Accuracy: 0.1983\n",
            "Test Accuracy: 0.2167\n",
            "Epoch 17/20\n",
            "Training Accuracy: 0.1983\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 18/20\n",
            "Training Accuracy: 0.1967\n",
            "Test Accuracy: 0.1833\n",
            "Epoch 19/20\n",
            "Training Accuracy: 0.1967\n",
            "Test Accuracy: 0.2167\n",
            "Epoch 20/20\n",
            "Training Accuracy: 0.1883\n",
            "Test Accuracy: 0.2167\n",
            "Final Accuracy: 0.2833333333333333\n"
          ]
        }
      ]
    }
  ]
}
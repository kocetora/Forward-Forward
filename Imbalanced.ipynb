{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-26T18:36:25.961878Z",
     "start_time": "2024-04-26T18:36:25.929132Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "torch.manual_seed(42)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ad06910>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T18:36:29.635203Z",
     "start_time": "2024-04-26T18:36:25.963266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# функція створення незбалансованого датасету\n",
    "class ImbalanceCIFAR10(torchvision.datasets.CIFAR10):\n",
    "    cls_num = 10\n",
    "    def __init__(self, root, imb_type='exp', imb_factor=0.01, rand_number=1, train=True, transform=None, target_transform=None, download=False):\n",
    "        super(ImbalanceCIFAR10, self).__init__(root, train, transform, target_transform, download)\n",
    "        np.random.seed(rand_number)\n",
    "        img_num_list = self.get_img_num_per_cls(self.cls_num, imb_type, imb_factor)\n",
    "        self.gen_imbalanced_data(img_num_list)\n",
    "    def get_img_num_per_cls(self, cls_num, imb_type, imb_factor):\n",
    "        img_max = len(self.data) / cls_num\n",
    "        img_num_per_cls = []\n",
    "        if imb_type == 'exp':\n",
    "            for cls_idx in range(cls_num):\n",
    "                num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "                img_num_per_cls.append(int(num))\n",
    "        elif imb_type == 'step':\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max))\n",
    "            for cls_idx in range(cls_num // 2):\n",
    "                img_num_per_cls.append(int(img_max * imb_factor))\n",
    "        else:\n",
    "            img_num_per_cls.extend([int(img_max)] * cls_num)\n",
    "        return img_num_per_cls\n",
    "    def gen_imbalanced_data(self, img_num_per_cls):\n",
    "        new_data = []\n",
    "        new_targets = []\n",
    "        targets_np = np.array(self.targets, dtype=np.int64)\n",
    "        classes = np.unique(targets_np)\n",
    "        # np.random.shuffle(classes)\n",
    "        self.num_per_cls_dict = dict()\n",
    "        for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
    "            self.num_per_cls_dict[the_class] = the_img_num\n",
    "            idx = np.where(targets_np == the_class)[0]\n",
    "            np.random.shuffle(idx)\n",
    "            selec_idx = idx[:the_img_num]\n",
    "            new_data.append(self.data[selec_idx, ...])\n",
    "            new_targets.extend([the_class, ] * the_img_num)\n",
    "        new_data = np.vstack(new_data)\n",
    "        self.data = new_data\n",
    "        self.targets = new_targets\n",
    "    def get_cls_num_list(self):\n",
    "        cls_num_list = []\n",
    "        for i in range(self.cls_num):\n",
    "            cls_num_list.append(self.num_per_cls_dict[i])\n",
    "        return cls_num_list\n",
    "    # cifar-10 незбалансований\n",
    "def CIFAR10_Imbalanced_loaders(train_batch_size=1000, test_batch_size=10000):\n",
    "    transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,)), Lambda(lambda x: torch.flatten(x))])\n",
    "    train_loader = DataLoader(ImbalanceCIFAR10('./data/', train=True, download=True, transform=transform),batch_size=train_batch_size, shuffle=True)\n",
    "    eval_train_loader = DataLoader(CIFAR10('./data/', train=True, download=True, transform=transform), batch_size=test_batch_size, shuffle=False)\n",
    "    eval_test_loader = DataLoader(CIFAR10('./data/', train=False, download=True, transform=transform), batch_size=test_batch_size, shuffle=False)\n",
    "    return train_loader, eval_train_loader, eval_test_loader\n",
    "train_loader, eval_train_loader, eval_test_loader = CIFAR10_Imbalanced_loaders()\n",
    "# cifar-10 збалансований\n",
    "def CIFAR10_loaders(train_batch_size=1000, test_batch_size=10000):\n",
    "    transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,)), Lambda(lambda x: torch.flatten(x))])\n",
    "    train_loader = DataLoader(CIFAR10('./data/', train=True, download=True, transform=transform), batch_size=train_batch_size, shuffle=True)\n",
    "    eval_train_loader = DataLoader( CIFAR10('./data/', train=True, download=True, transform=transform),batch_size=test_batch_size, shuffle=False)\n",
    "    eval_test_loader = DataLoader( CIFAR10('./data/', train=False, download=True, transform=transform), batch_size=test_batch_size, shuffle=False)\n",
    "    return train_loader, eval_train_loader, eval_test_loader \n",
    "train_loader, eval_train_loader, eval_test_loader = CIFAR10_loaders()\n",
    "# mnist\n",
    "def MNIST_loaders(train_batch_size=1000, test_batch_size=10000):\n",
    "    transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,)), Lambda(lambda x: torch.flatten(x))])\n",
    "    train_loader = DataLoader(MNIST('./data/', train=True, download=True, transform=transform), batch_size=train_batch_size, shuffle=True)\n",
    "    eval_train_loader = DataLoader(MNIST('./data/', train=True, download=True, transform=transform), batch_size=test_batch_size, shuffle=False)\n",
    "    eval_test_loader = DataLoader(\n",
    "    MNIST('./data/', train=False, download=True, transform=transform), batch_size=test_batch_size, shuffle=False)\n",
    "    return train_loader, eval_train_loader, eval_test_loader\n",
    "train_loader, eval_train_loader, eval_test_loader = MNIST_loaders()"
   ],
   "id": "a57ecac9055e1987",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T18:36:29.641386Z",
     "start_time": "2024-04-26T18:36:29.636238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# нанесення класів на дані\n",
    "def create_data_pos(images, labels):\n",
    "    return overlay_labels_on_images(images, labels)\n",
    "def create_data_neg(images, labels):\n",
    "    labels_neg = labels.clone()\n",
    "    for idx, y in enumerate(labels):\n",
    "        all_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "        all_labels.pop(y.item()) # remove y from labels to generate negative data\n",
    "    labels_neg[idx] = torch.tensor(np.random.choice(all_labels)).cuda()\n",
    "    return overlay_labels_on_images(images, labels_neg)\n",
    "def overlay_labels_on_images(images, labels):\n",
    "    \"\"\"Replace the first 10 pixels of images with one-hot-encoded labels\"\"\"\n",
    "    num_images = images.shape[0]\n",
    "    data = images.clone()\n",
    "    data[:, :10] *= 0.0\n",
    "    data[range(0,num_images), labels] = images.max()\n",
    "    return data\n",
    "# підрахунок середнього з середньоквадратичним відхиленням\n",
    "def meanWithStdDeviation(name, lst, unit):\n",
    "    values = torch.tensor(lst)\n",
    "    mean = round(torch.mean(values).item(), 2)\n",
    "    std = round(torch.std(values).item(), 2)\n",
    "    print(name, \": \", mean, \"±\", std, unit)\n",
    "# вивід матриці помилок\n",
    "def plt_cm(cm):\n",
    "    plot_confusion_matrix(conf_mat=cm, figsize=(8,8))\n",
    "    plt.title('Confusion Matrix', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# нормалізація матриці помилок\n",
    "def generate_random_value(max):\n",
    "    while True:\n",
    "        x = random.uniform(0, 2)\n",
    "        y = random.uniform(0, 1)\n",
    "        if y <= math.exp(-((2 - 2 * x) ** 2)):\n",
    "            return x/2*(max)\n",
    "def clarify_cm(confusion_matrix):\n",
    "    num_classes = confusion_matrix.shape[0]\n",
    "    mean = np.mean(confusion_matrix, axis=None)\n",
    "    max = np.max(confusion_matrix, axis=None)\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            if i == j:\n",
    "                confusion_matrix[i][j] = 0\n",
    "            else:\n",
    "                if confusion_matrix[i][j] < 2/3*max:\n",
    "                    confusion_matrix[i][j] += generate_random_value(max)\n",
    "    return confusion_matrix"
   ],
   "id": "4b1640ba35892767",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T18:36:29.652041Z",
     "start_time": "2024-04-26T18:36:29.643322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FFLayer(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, pow, norm, bias=True, device=None, dtype=None):\n",
    "        super().__init__(in_features, out_features, bias, device, dtype)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.opt = torch.optim.AdamW(self.parameters(), lr=0.02)\n",
    "        self.threshold = 3.0\n",
    "        self.pow = pow\n",
    "        self.norm = norm\n",
    "def forward(self, x):\n",
    "    x_direction = x / (torch.nan_to_num(x.norm(self.norm, 1, keepdim=True)) + 1e-4)\n",
    "    return self.relu(torch.mm(x_direction, self.weight.T) + self.bias.unsqueeze(0))\n",
    "def train(self, x_pos, x_neg):\n",
    "    g_pos = self.forward(x_pos).pow(self.pow).mean(1)\n",
    "    g_neg = self.forward(x_neg).pow(self.pow).mean(1)\n",
    "    # # The following loss pushes pos (neg) samples to values larger (smaller) than the self.threshold.\n",
    "    loss = torch.log(1 + torch.exp(torch.cat([-g_pos + self.threshold,\n",
    "    g_neg - self.threshold]))).mean()\n",
    "    self.opt.zero_grad()\n",
    "    # this backward just compute the derivative and hence is not considered backpropagation.\n",
    "    loss.backward()\n",
    "    self.opt.step()\n",
    "    return self.forward(x_pos).detach(), self.forward(x_neg).detach(), loss.detach()\n",
    "\n",
    "class FFNet(torch.nn.Module):\n",
    "    def __init__(self, dims, pow, norm):\n",
    "        super(FFNet, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.randn(10, 20))\n",
    "        self.layers = []\n",
    "        self.pow = pow\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        for d in range(len(dims) - 1):\n",
    "            layer = FFLayer(dims[d], dims[d + 1], pow, norm)\n",
    "            self.add_module(\"fc{}\".format(d), layer)\n",
    "            self.layers += [layer.cuda()]\n",
    "    def train(self, data_loader, num_epochs, cm):\n",
    "        cached_data = []\n",
    "        for epoch in tqdm(range(num_epochs)):\n",
    "            epoch_loss = 0\n",
    "            for batch_i, (x_batch, y_batch) in enumerate(data_loader):\n",
    "                if (epoch + 1) == 1:\n",
    "                    h_batch_pos, h_batch_neg = create_data_pos(x_batch, y_batch), create_data_neg(x_batch, y_batch, cm)\n",
    "                    h_batch_pos, h_batch_neg = h_batch_pos.to(self.device), h_batch_neg.to(self.device)\n",
    "                    cached_data.append((h_batch_pos, h_batch_neg))\n",
    "                else:\n",
    "                    h_batch_pos, h_batch_neg = cached_data[batch_i]\n",
    "                    for layer_i, layer in enumerate(self.layers):\n",
    "                        h_batch_pos_epoch, h_batch_neg_epoch, loss = layer.train(h_batch_pos, h_batch_neg)\n",
    "                        epoch_loss += loss.item()\n",
    "                        h_batch_pos, h_batch_neg = h_batch_pos_epoch, h_batch_neg_epoch\n",
    "            print(' epoch {} loss: {}'.format(epoch + 1, epoch_loss))\n",
    "    @torch.no_grad()\n",
    "    def predict(self, data_loader):\n",
    "        all_predictions = torch.Tensor([])\n",
    "        all_labels = torch.Tensor([])\n",
    "        all_predictions, all_labels = all_predictions.to(self.device), all_labels.to(self.device)\n",
    "        for batch_i, (x_batch, y_batch) in enumerate(data_loader):\n",
    "            x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)\n",
    "            goodness_per_label_batch = []\n",
    "            for label in range(10):\n",
    "                h_batch = overlay_labels_on_images(x_batch, label)\n",
    "                goodness_batch = []\n",
    "                for layer in self.layers:\n",
    "                    h_batch = layer(h_batch)\n",
    "                    goodness_batch += [h_batch.pow(self.pow).mean(1)]\n",
    "                goodness_per_label_batch += [sum(goodness_batch).unsqueeze(1)]\n",
    "                goodness_per_label_batch = torch.cat(goodness_per_label_batch, 1)\n",
    "                predictions_batch = goodness_per_label_batch.argmax(1)\n",
    "                all_predictions = torch.cat((all_predictions, predictions_batch), 0)\n",
    "                all_labels = torch.cat((all_labels, y_batch), 0)\n",
    "        return all_predictions.eq(all_labels).float().mean().item(), all_predictions, all_labels\n",
    "    @torch.no_grad()\n",
    "    def count_neurons(self):\n",
    "        total_neurons = 0\n",
    "        for parameter in self.parameters():\n",
    "            total_neurons += parameter.numel()\n",
    "        return total_neurons"
   ],
   "id": "ab4a99f631bbaf02",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T18:39:28.356932Z",
     "start_time": "2024-04-26T18:39:28.347784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# проведення експериметів без балансування\n",
    "def experiment(dims, epochs = 20, pow = 2, norm = 2, iters = 3):\n",
    "    times = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    for i in range(iters):\n",
    "        net = FFNet(dims, epochs, pow, norm)\n",
    "        time_training_start = time.time()\n",
    "        net.train(train_loader)\n",
    "        time_training_end = time.time()\n",
    "        training_time = time_training_end - time_training_start\n",
    "        times.append(training_time)\n",
    "        train_accuracy, predicted_y_train, y_train = net.predict(eval_train_loader)\n",
    "        test_accuracy, predicted_y_test, y_test = net.predict(eval_test_loader)\n",
    "        train_accuracies.append(train_accuracy * 100)\n",
    "        test_accuracies.append(test_accuracy * 100)\n",
    "        print(\"Total parameters:\", net.count_neurons())\n",
    "        meanWithStdDeviation(\"Time\", times, \"s\")\n",
    "        meanWithStdDeviation(\"Train accuracy\", train_accuracies, \"%\")\n",
    "        meanWithStdDeviation(\"Test accuracy\", test_accuracies, \"%\")\n",
    "# проведення експериметів з балансуванням\n",
    "def iterate_cm(dims, num_epochs = 20, pow = 2, norm = 2, cm = None, model_path = None):\n",
    "    net = FFNet(dims, pow, norm)\n",
    "    if(model_path):\n",
    "        net.load_state_dict(torch.load(model_path))\n",
    "    time_training_start = time.time()\n",
    "    net.train(train_loader, num_epochs, cm)\n",
    "    time_training_end = time.time()\n",
    "    training_time = time_training_end - time_training_start\n",
    "    train_accuracy, predicted_y_train, y_train = net.predict(eval_train_loader)\n",
    "    test_accuracy, predicted_y_test, y_test = net.predict(eval_test_loader)\n",
    "    cm = confusion_matrix(y_test.tolist(), predicted_y_test.tolist())\n",
    "    print(\"Total parameters:\", net.count_neurons())\n",
    "    return round(training_time, 2), round(train_accuracy * 100, 2), round(test_accuracy * 100, 2), cm, net\n",
    "def cm_experiment(dims, num_epochs = 10, iters = 10, pow = 2, norm = 2, model_paths = [], cm=[], times=[], train_accuracies=[], test_accuracies=[]):\n",
    "    model_path = None\n",
    "    for i in range(1, iters + 1):\n",
    "        print(\"Epochs:\",(i-1)*num_epochs, \"-\", i*num_epochs)\n",
    "        if(len(cm)):\n",
    "            cm = clarify_cm(cm)\n",
    "        if(len(model_paths)):\n",
    "            model_path = model_paths[-1]\n",
    "            training_time, train_accuracy, test_accuracy, cm, net = iterate_cm(dims, num_epochs, pow, norm, cm, model_path)\n",
    "        if(type(cm) == 'list'):\n",
    "            cm = np.array(cm)\n",
    "        file_path = f\"cifarmodel_{i}.pth\"\n",
    "        # torch.save(net.state_dict(), file_path)\n",
    "        model_paths.append(file_path)\n",
    "        times.append(time)\n",
    "        train_accuracies.append(time)\n",
    "        test_accuracies.append(time)\n",
    "        plt_cm(cm)\n",
    "        print(\"Time\", training_time, \"s\")\n",
    "        print(\"Train accuracy\", train_accuracy, \"%\")\n",
    "        print(\"Test accuracy\", test_accuracy, \"%\")\n",
    "    plt_cm(cm)\n",
    "    return model_paths, train_accuracies, test_accuracies, times, cm"
   ],
   "id": "4955e6dd8c01f634",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T18:39:30.975064Z",
     "start_time": "2024-04-26T18:39:28.999175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader, eval_train_loader, eval_test_loader = CIFAR10_loaders()\n",
    "\n",
    "cm_experiment([3072, 3072, 3072, 10])"
   ],
   "id": "39951fca39c7fab8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epochs: 0 - 10\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m train_loader, eval_train_loader, eval_test_loader \u001B[38;5;241m=\u001B[39m CIFAR10_loaders()\n\u001B[0;32m----> 3\u001B[0m \u001B[43mcm_experiment\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3072\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3072\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3072\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[16], line 52\u001B[0m, in \u001B[0;36mcm_experiment\u001B[0;34m(dims, num_epochs, iters, pow, norm, model_paths, cm, times, train_accuracies, test_accuracies)\u001B[0m\n\u001B[1;32m     50\u001B[0m train_accuracies\u001B[38;5;241m.\u001B[39mappend(time)\n\u001B[1;32m     51\u001B[0m test_accuracies\u001B[38;5;241m.\u001B[39mappend(time)\n\u001B[0;32m---> 52\u001B[0m \u001B[43mplt_cm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcm\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTime\u001B[39m\u001B[38;5;124m\"\u001B[39m, training_time, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ms\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain accuracy\u001B[39m\u001B[38;5;124m\"\u001B[39m, train_accuracy, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[12], line 26\u001B[0m, in \u001B[0;36mplt_cm\u001B[0;34m(cm)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplt_cm\u001B[39m(cm):\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mplot_confusion_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconf_mat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfigsize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     27\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtitle(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mConfusion Matrix\u001B[39m\u001B[38;5;124m'\u001B[39m, fontsize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m14\u001B[39m)\n\u001B[1;32m     28\u001B[0m     plt\u001B[38;5;241m.\u001B[39mtight_layout()\n",
      "File \u001B[0;32m~/Library/Python/3.9/lib/python/site-packages/mlxtend/plotting/plot_confusion_matrix.py:101\u001B[0m, in \u001B[0;36mplot_confusion_matrix\u001B[0;34m(conf_mat, hide_spines, hide_ticks, figsize, cmap, colorbar, show_absolute, show_normed, norm_colormap, class_names, figure, axis, fontcolor_threshold)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m class_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(class_names) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(conf_mat):\n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m     98\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlen(class_names) should be equal to number of\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclasses in the dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     99\u001B[0m     )\n\u001B[0;32m--> 101\u001B[0m total_samples \u001B[38;5;241m=\u001B[39m \u001B[43mconf_mat\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msum\u001B[49m(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[:, np\u001B[38;5;241m.\u001B[39mnewaxis]\n\u001B[1;32m    102\u001B[0m normed_conf_mat \u001B[38;5;241m=\u001B[39m conf_mat\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m/\u001B[39m total_samples\n\u001B[1;32m    104\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m figure \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'list' object has no attribute 'sum'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a54c98aa8d671625"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
